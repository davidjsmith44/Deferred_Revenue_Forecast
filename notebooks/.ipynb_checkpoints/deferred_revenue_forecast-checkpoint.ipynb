{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deferred Revenue Forecast\n",
    "\n",
    "This jupyter notebook contains the details Adobe's deferred revenue forecast model in a more readable, easy to understand format.\n",
    "\n",
    "The program is run from python directly (versus using a notebook like we have here) and the details are available on my github page www.github.com/davidjsmith44/Deferred_Revenue_Forecast\n",
    "Note: You will need me to give you access to this repository as it is not public\n",
    "\n",
    "\n",
    "Steps to the program\n",
    "1. Load up all input data\n",
    "    - billings history\n",
    "        - Type A\n",
    "    - FX rates\n",
    "    - FX_currency map\n",
    "    - FX forwards\n",
    "    - bookings data\n",
    " \n",
    " \n",
    "2. Process the billings data into a dataframe that includes the BU, currency, period and every type of billings based on it's rebill frequency\n",
    " \n",
    "3. Process the bookings information\n",
    "\n",
    "4. Forecast the future billings\n",
    "\n",
    "5. Basic reporting documents\n",
    "\n",
    "6. Checking for sanity\n",
    "\n",
    "\n",
    "NOTE: If you plan on using a Jupyter Notebook to run the Deferred Revenue Forecast, you must first download all of the input data to your local directory. I do not yet know how to use a Jupyter Notebook and access data off of the Treasury server.\n",
    "\n",
    "The input data sits on the Treasury server under Treasury\\Financial_Database\\Deferred_Revenue\\Inputs\\Data_YYYY_pMM\n",
    "\n",
    "There will be 6 files located at this directory that need to be copied to your local drive (preferably where you have installed Anaconda) under a directory called data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Begins here\n",
    "Below are standard import statements to include all the functionality of numpy, pandas, matplotlib, pickle and a linear regression moodel from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "from math import ceil\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.interpolate import interp1d, griddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Processing Base Billings Data\n",
    "\n",
    "The billings data comes from tableau and is saved in a file \"all_billings_inputs.xlsx\" with two sheets of information\n",
    "\n",
    "\"base_billings\"\n",
    "Contains the basic information about all of the billings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/Data_2020_P06/all_billings_inputs.xlsx', sheet_name='base_billings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Changing the column names early since they are inconsistent across other reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(index = str, columns = {'Document Currency': 'curr',\n",
    "                                 'Enterprise BU Desc': 'BU',\n",
    "                                 'Invoice Fiscal Year Period Desc': 'period',\n",
    "                                 'Product Config Type': 'config',\n",
    "                                 'Rev Rec Category': 'rev_req_type',\n",
    "                                 'Rule For Bill Date': 'rebill_rule',\n",
    "                                 'Completed Sales ( DC )': 'DC_amount',\n",
    "                                 'Completed Sales': 'US_amount'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To inspect the dataframe we just loaded, you can remove the '#' symbol from the rows below. The # symbol makes the line of code a comment (so the program will not read and execute that line of code). Removing the # will make the line executable and display the results\n",
    "\n",
    "The df.head(5) command will show the first 5 rows of the billings dataframe. To run this, simply remove the # before this line\n",
    " \n",
    "The df.tail(5) command will show the last 5 rows of the billings dataframe. To run this, simply remove the # before this line\n",
    "\n",
    "The df.sample(5) command will show a random 5 rows of the rec dataframe. To run this, simply remove the # before this line\n",
    "\n",
    "Note 1: Only one of these commands can be entered in a single code cell at a time. (So if you remove 2 of the #, only one of the commands will be displayed.\n",
    "\n",
    "Note 2: The 5 can also be changed, but I think it caps out at 100. For example df.head(30) will show the first 30 rows of the billings dataframe.\n",
    "\n",
    "Note 3: The code must sit on the far leftmost column of the code window. If there is a space it will cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(10)\n",
    "#df.tail(5)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out any currency that has  < 10 transactions. \n",
    "###### To see the list of currencies and how any times they appear in the billings database remove the # before the print(vc) statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of the currencies and the number of transactions for each currency\n",
    "vc = df['curr'].value_counts()\n",
    "#print(vc)\n",
    "\n",
    "#Create variable that is true if the number of transaction is greater than 20, false otherwise\n",
    "keep_these = vc.values > 20\n",
    "# filtering only currencies that were greater than 20\n",
    "keep_curr = vc[keep_these]\n",
    "a = keep_curr.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just keeping track of the currencies we removed in our model_dict data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_these = vc[vc.values <= 20].index\n",
    "model_dict = {'curr_removed': list(vc[remove_these].index)}\n",
    "delete_curr = list(remove_these)\n",
    "print(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FX database does not have information on the following currencies\n",
    " - AED (United Arab Emirates Dirham)\n",
    " - BMD (Bermudan Dollar)\n",
    " - MXP (Mexican Peso)\n",
    " - TRY (Turkish Lira)\n",
    " \n",
    " Below we are adding the Turkish Lira to our list of currencies that should be removed from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TRY' not in model_dict['curr_removed']:\n",
    "    model_dict['curr_removed'].append('TRY')\n",
    "    delete_curr.append('TRY')\n",
    "    a = a.drop('TRY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Clearing out the infrequent currencies from our billings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['curr'].isin(a)]\n",
    "\n",
    "print('Model dictionary', model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---Removing infrequent currencies from billings history---\")\n",
    "print('Total number of currencies in the base billings file: ', len(vc))\n",
    "if len(model_dict['curr_removed'])==0:\n",
    "    print('No currencies were removed, all contained 10 or more billings')\n",
    "    print('Currencies in the base billings file')\n",
    "    for item in a:\n",
    "        print(a[item], end = \" \")\n",
    "else:\n",
    "    print('\\n Currencies were removed: ', len(model_dict['curr_removed']))\n",
    "\n",
    "    for item in remove_these:\n",
    "        print(item, ', ', end=\"\")\n",
    "        \n",
    "    print(\"\\n\\n{} Remaining currencies: \".format(len(a)))\n",
    "    for item in a:\n",
    "        print(item, ', ', end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing any of the values that are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the length of the dataframe before removing zeros: ', len(df))\n",
    "df = df[df['DC_amount']!=0]\n",
    "print('This is the length of the dataframe after removing zeros: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "#df.tail(10)\n",
    "#df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearing out the Non-Revenue billings from the file\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sales Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of the dataframe before removing non-revenue billings: ', len(df))\n",
    "df = df[df['Sales Type']!='NON-REV']\n",
    "print('Length of the dataframe after removing non-revenue billings:  ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Grouping the billings by sales type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the data by the <b> Sales Type </b> field\n",
    " - <i>'RECOGNIZED'</i> sales are perpetual and go straight to revenue without hitting deferred \n",
    " - <i>'PRO-SVC-INV'</i> professional services that are invoiced and go to revenue directly when invoiced\n",
    " - <i>'DEFERRED'</i> sales that will sit on the balance sheet in deferred revenue and amortize over their life\n",
    " \n",
    " #### Below we are creating a seperate dataframe for each of the Sales Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = df[df['Sales Type']=='RECOGNIZED'].copy()\n",
    "svc = df[df['Sales Type']=='PRO-SVC-INV'].copy()\n",
    "dfr = df[df['Sales Type']=='DEFERRED'].copy()\n",
    "\n",
    "print('Total number of billings:              ', len(df))\n",
    "print(\"Number of recognized revenue billings: \", len(rec))\n",
    "print(\"Number of service invoiced billings:   \", len(svc))\n",
    "print(\"Number of deferred revenue billings:   \", len(dfr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognized Revenue\n",
    "\n",
    "The rec.head(5) command will show the first 5 elements of the rec dataframe. To run this, simply remove the # before this line\n",
    " \n",
    "The rec.tail(5) command will show the last 5 elements of the rec dataframe. To run this, simply remove the # before this line\n",
    "\n",
    "The rec.sample(5) command will show the a random 5 elements of the rec dataframe. To run this, simply remove the # before this line\n",
    "\n",
    "Note: Only one of these commands can be entered in a single code cell at a time. \n",
    "\n",
    "Note: The 5 can also be changed, but I think it caps out at 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.head(5)\n",
    "#rec.tail(5)\n",
    "#rec.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below we are grouping the rec dataframe by Currency, Business Unit and Period and cleaning up the data we do not need. Since the recognized revenue go directly to revenue, there is no contract that will renew and need to be modeled in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing groupby object\n",
    "gb_rec = rec.groupby(['curr', 'BU', 'period'], as_index=False).sum()\n",
    "gb_rec.drop(labels='Subscription Term', axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_rec.head(10)\n",
    "#gb_rec.tail(10)\n",
    "#gb_rec.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service Billings\n",
    "\n",
    "##### Below we are grouping the svc dataframe by Currency, Business Unit and Period and cleaning up the data we do not need. Since the service billings go directly to revenue, there is no contract that will renew and need to be modeled in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_svc = svc.groupby(['curr', 'BU', 'period'], as_index=False).sum()\n",
    "gb_svc.drop(labels='Subscription Term', axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_svc.head(15)\n",
    "#gb_svc.tail(5)\n",
    "#gb_svc.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffered Billings\n",
    "\n",
    "#### Type B Billings \n",
    "Type B billings are service agreements that will have invoices submitted before the billings are reclassified to revenue. If no invoices are assigned to the billings, the billings become revenue in 12 months. Since these billings do not have a contract that will renew in the future, there is no need to model a rebillings of these service based billings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_b = dfr[dfr['rev_req_type']=='B'].copy()\n",
    "gb_b = dfr_b.groupby(['curr', 'BU', 'period'], as_index=False).sum()\n",
    "gb_b.drop(labels='Subscription Term', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_b.head(20)\n",
    "#gb_b.tail(25)\n",
    "#gb_b.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type A Billings\n",
    "\n",
    "These billings are on a billing plan. The product config tells us how long before they renew\n",
    "\n",
    " - '3Y' = 36 months\n",
    " - '2Y' = 24 months\n",
    " - '1Y' = 12 months\n",
    " - 'MTHLY' = 1 month\n",
    " \n",
    "NOTE: There are also other fields in the 'Product Configtype ID' field that do not map well to a rebill period.\n",
    "To fix this, we need to load up a different file and determine the length of the sales contract (type A no config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_a = dfr[dfr['rev_req_type']=='A'].copy()\n",
    "\n",
    "gb_a = dfr_a.groupby(['curr', 'BU', 'period',\n",
    "                     'config'], as_index=False).sum()\n",
    "gb_a.drop(labels='Subscription Term', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb_a.head(20)\n",
    "#gb_a.tail(20)\n",
    "#gb_a.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_a['config'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is just a check to see how large the billing types are across all periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_a_config = gb_a.groupby(['config'], as_index=False).sum()\n",
    "gb_a_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### These 'OCONS', 'OENSV', 'ONORE' and 'OUNIV' config types are not actual product config IDs so we have to get them from a different data file. We are excluding these types below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = ['1Y', '2Y', '3Y', 'MTHLY']\n",
    "gb_a_config = gb_a[gb_a['config'].isin(config_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grouping by the config type into gb_a_1Y, gb_a_2Y, gb_a_3y, gb_a_1M dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_a_1Y = gb_a_config[gb_a_config['config']=='1Y'].copy()\n",
    "gb_a_2Y = gb_a_config[gb_a_config['config']=='2Y'].copy()\n",
    "gb_a_3Y = gb_a_config[gb_a_config['config']=='3Y'].copy()\n",
    "gb_a_1M = gb_a_config[gb_a_config['config']=='MTHLY'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('this is the lenght of type A 1M billings: ', len(gb_a_1M))\n",
    "print('this is the lenght of type A 1Y billings: ', len(gb_a_1Y))\n",
    "print('this is the lenght of type A 2Y billings: ', len(gb_a_2Y))\n",
    "print('this is the lenght of type A 3Y billings: ', len(gb_a_3Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb_a_2Y.head(5)\n",
    "#gb_a_1M.tail(5)\n",
    "#gb_a_3Y.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TYPE D billings\n",
    "These billings have a field 'Rule For Bill Date' that determines when new billings will occur\n",
    " - Monthly:        *{Y1, Y2, Y3, Y5}*\n",
    " - Quarterly:      *YQ*\n",
    " - Every 4 months: *YT*  --NOTE: There are only 10 of these, so I am treating these as quarterly--\n",
    " - Semi-annual:    *YH*\n",
    " - Annual:         *{YA, YC}*\n",
    " - Every 2 years:  *Y4*\n",
    " - Every 3 years:  *Y7*\n",
    " \n",
    " We also need to track the type D billings that do not have a 'Rule for Bill Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_d = dfr[dfr['rev_req_type']=='D'].copy()\n",
    "\n",
    "gb_d = dfr_d.groupby(['curr', 'BU', 'period',\n",
    "                     'rebill_rule'], as_index=False).sum()\n",
    "gb_d.drop(labels='Subscription Term', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_d['rebill_rule'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grouping these by rebill rule and incorporating rebill rules that have the same rebill period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_d_mthly = gb_d[gb_d['rebill_rule'].isin(['Y1', 'Y2', 'Y3', 'YM'])].copy()\n",
    "gb_d_mthly.drop(labels='rebill_rule', axis=1,inplace=True)\n",
    "gb_d_mthly = gb_d_mthly.groupby(['curr', 'BU', 'period']).sum()\n",
    "gb_d_mthly.reset_index(inplace=True)\n",
    "\n",
    "gb_d_qtrly = gb_d[gb_d['rebill_rule'].isin(['YQ', 'YY', 'YT'])].copy()\n",
    "gb_d_qtrly.drop(labels='rebill_rule', axis=1,inplace=True)\n",
    "gb_d_qtrly = gb_d_qtrly.groupby(['curr', 'BU', 'period']).sum()\n",
    "gb_d_qtrly.reset_index(inplace=True)\n",
    "\n",
    "gb_d_semi_ann = gb_d[gb_d['rebill_rule']=='YH']\n",
    "\n",
    "gb_d_annual = gb_d[gb_d['rebill_rule'].isin(['YA', 'YC', 'YX'])].copy()\n",
    "gb_d_annual.drop(labels='rebill_rule', axis=1,inplace=True)\n",
    "gb_d_annual = gb_d_annual.groupby(['curr', 'BU', 'period']).sum()\n",
    "gb_d_annual.reset_index(inplace=True)\n",
    "\n",
    "gb_d_two_yrs = gb_d[gb_d['rebill_rule']=='Y4']\n",
    "gb_d_three_yrs = gb_d[gb_d['rebill_rule']=='Y7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb_d_qtrly.head(10)\n",
    "#gb_d_annual.tail(10)\n",
    "#gb_d_three_yrs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of monthly', len(gb_d_mthly))\n",
    "print('Length of quarterly', len(gb_d_qtrly))\n",
    "print('Length of semi ann', len(gb_d_semi_ann))\n",
    "print('Length of annual', len(gb_d_annual))\n",
    "print('Length of two years', len(gb_d_two_yrs))\n",
    "print('Length of three years', len(gb_d_three_yrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a single dataframe that incorporates all of this data\n",
    "\n",
    "- We will have the following descriptive fields\n",
    "   - Invoicing Fiscal Year-Period\n",
    "   - Document Currency Billing Amount\n",
    "   - USD Billing Amount\n",
    "   - Enterprise BU\n",
    "\n",
    "- We will have the following fields based on rebilling rule\n",
    "   - Recognized\n",
    "   - Service\n",
    "   - Monthly\n",
    "   - Quarterly\n",
    "   - Annual\n",
    "   - Two Years\n",
    "   - Three Years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below uses functions to merge a list of dataframes and move billings amounts to the correct category based on rebill frequency and type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = [gb_rec, gb_svc, gb_b,\n",
    "           gb_a_1M,    gb_a_1Y,    gb_a_2Y,       gb_a_3Y, \n",
    "           gb_d_mthly, gb_d_qtrly, gb_d_semi_ann, gb_d_annual, gb_d_two_yrs, gb_d_three_yrs]\n",
    "\n",
    "list_columns = ['recognized', 'service', 'deferred_B', \n",
    "        'deferred_1M_a', 'deferred_1Y_a', 'deferred_2Y_a', 'deferred_3Y_a',\n",
    "        'deferred_1M_d', 'deferred_3M_d', 'deferred_6M_d', 'deferred_1Y_d', 'deferred_2Y_d', 'deferred_3Y_d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_USD_amt(list_df, list_columns):\n",
    "    total_US = []\n",
    "    for df in list_df:\n",
    "        total_US.append(df['US_amount'].sum())\n",
    "    total_df = pd.DataFrame(index = list_columns, columns = ['US_amounts'], data=total_US)\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_dataframes(list_df, list_columns):\n",
    "    for i, df in enumerate(list_df):\n",
    "        #print('This is i:', i)\n",
    "        #print('referencing the column: ', list_columns[i])\n",
    "\n",
    "        if i==0:\n",
    "            df_merged = list_df[0].copy()\n",
    "            df_merged.rename(index=str, columns={'DC_amount': list_columns[i]+'_DC', \n",
    "                                                 'US_amount': list_columns[i]+'_US'}, inplace=True)\n",
    "        else:\n",
    "            df_merged = merge_new_dataframe(df_merged, df, list_columns[i])\n",
    "\n",
    "    return df_merged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_new_dataframe(old_df, new_df, new_column):\n",
    "    df_merged = pd.merge(old_df, new_df, how='outer', \n",
    "                     left_on=['curr', 'BU', 'period'],\n",
    "                    right_on=['curr', 'BU', 'period'])\n",
    "    df_merged.rename(index=str, columns={'DC_amount': new_column+'_DC', 'US_amount': new_column+'_US'}, inplace=True)\n",
    "    \n",
    "    #need to drop the product configtype id for merges where the new_df is of type A\n",
    "    config_str = 'config'\n",
    "    rule_str = 'rebill_rule'\n",
    "    if config_str in df_merged.columns:\n",
    "        df_merged.drop(columns=['config'], inplace=True)\n",
    "    \n",
    "    if rule_str in df_merged.columns:\n",
    "        df_merged.drop(columns=['rebill_rule'], inplace=True)\n",
    "        \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_columns(df):\n",
    "    \n",
    "    # clean up NaNs before adding \n",
    "    df = df.fillna(value=0)\n",
    "    \n",
    "    # DC columns first\n",
    "    # Monthly\n",
    "    df['deferred_1M_DC'] = df['deferred_1M_a_DC']+df['deferred_1M_d_DC']\n",
    "    df.drop(labels=['deferred_1M_a_DC', 'deferred_1M_d_DC'], axis=1, inplace=True)\n",
    "    \n",
    "    # Annual\n",
    "    df['deferred_1Y_DC'] = df['deferred_1Y_a_DC']+df['deferred_1Y_d_DC']\n",
    "    df.drop(labels=['deferred_1Y_a_DC', 'deferred_1Y_d_DC'], axis=1, inplace=True)\n",
    "    \n",
    "    # Two-Year\n",
    "    df['deferred_2Y_DC'] = df['deferred_2Y_a_DC']+df['deferred_2Y_d_DC']\n",
    "    df.drop(labels=['deferred_2Y_a_DC', 'deferred_2Y_d_DC'], axis=1, inplace=True)\n",
    "    \n",
    "    #Three-Year\n",
    "    df['deferred_3Y_DC'] = df['deferred_3Y_a_DC']+df['deferred_3Y_d_DC']\n",
    "    df.drop(labels=['deferred_3Y_a_DC', 'deferred_3Y_d_DC'], axis=1, inplace=True)\n",
    "    \n",
    "    # renaming 3M and 6M\n",
    "    df.rename(index=str, columns = {'deferred_3M_d_DC':'deferred_3M_DC', \n",
    "                               'deferred_6M_d_DC': 'deferred_6M_DC'}, inplace=True)\n",
    "\n",
    "    # US columns\n",
    "    # Monthly\n",
    "    df['deferred_1M_US'] = df['deferred_1M_a_US']+df['deferred_1M_d_US']\n",
    "    df.drop(labels=['deferred_1M_a_US', 'deferred_1M_d_US'], axis=1, inplace=True)\n",
    "    \n",
    "    # Annual\n",
    "    df['deferred_1Y_US'] = df['deferred_1Y_a_US']+df['deferred_1Y_d_US']\n",
    "    df.drop(labels=['deferred_1Y_a_US', 'deferred_1Y_d_US'], axis=1, inplace=True)\n",
    "    \n",
    "    # Two-Year\n",
    "    df['deferred_2Y_US'] = df['deferred_2Y_a_US']+df['deferred_2Y_d_US']\n",
    "    df.drop(labels=['deferred_2Y_a_US', 'deferred_2Y_d_US'], axis=1, inplace=True)\n",
    "    \n",
    "    # Three-Year\n",
    "    df['deferred_3Y_US'] = df['deferred_3Y_a_US']+df['deferred_3Y_d_US']\n",
    "    df.drop(labels=['deferred_3Y_a_US', 'deferred_3Y_d_US'], axis=1, inplace=True)\n",
    "    \n",
    "    # renaming 3M and 6M\n",
    "    df.rename(index=str, columns = {'deferred_3M_d_US':'deferred_3M_US', \n",
    "                               'deferred_6M_d_US': 'deferred_6M_US'}, inplace=True)\n",
    "\n",
    "    \n",
    "    #cleaning up the longer column names\n",
    "    df.rename(index=str, columns = {'curr': 'curr',\n",
    "                               'BU':'BU',\n",
    "                               'period':'period'}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The code below uses the functions above to merge all of the dataframes and clean up the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_all_dataframes(list_df, list_columns)\n",
    "\n",
    "df = clean_df_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I NEED TO CREATE A BETTER PRESENTATION OF THIS CHECK THAT EVERYTHING MATCHES!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to create a summary report with totals coming from every area to make sure the totals I have make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = sum_USD_amt(list_df, list_columns)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc['deferred_1M_d']+total_df.loc['deferred_1M_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this a function to be cleaned up somehow\n",
    "del dfr\n",
    "del dfr_a\n",
    "del dfr_b\n",
    "del dfr_d\n",
    "del gb_a\n",
    "del gb_a_1M\n",
    "del gb_a_1Y\n",
    "del gb_a_2Y\n",
    "del gb_a_3Y\n",
    "del gb_b, \n",
    "del gb_d\n",
    "del gb_svc, gb_rec, gb_d_two_yrs\n",
    "del gb_d_qtrly, gb_d_semi_ann\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE DONE:\n",
    "\n",
    "1. Clean up the type F billings (at least check to see if they are necessary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading up the Adobe Financial Calendar to get period start and end dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Adobe financial calendar and calculating period weeks\n",
    "df_cal = pd.read_excel('../data/old/ADOBE_FINANCIAL_CALENDAR.xlsx', 'ADBE_cal')\n",
    "df_cal['Period_Weeks'] = (df_cal['Per_End']-df_cal['Per_Start'])/np.timedelta64(1, 'W')\n",
    "df_cal['Period_Weeks']=df_cal['Period_Weeks'].astype(int)\n",
    "df_cal['Period_Weeks'] = df_cal['Period_Weeks']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cal.head(5)\n",
    "#df_cal.sample(5)\n",
    "df_cal.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Type A No Config Type Billings\n",
    "___\n",
    "\n",
    "This file contains type A billings that have a revenue contract start date and end date. We need to map these into the terms of our dataframe.\n",
    "\n",
    "#### Steps\n",
    "1. Rename the columns\n",
    "2. This file has entries for pennies. Need to clear out anything less than $10 in absolute value\n",
    "3. Determine the length of time between start date and end date\n",
    "4. Group this dataframe by currency, period and BU\n",
    "5. Merge this final dataframe with the larger dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: This file contains two different start date and end date columns. At least one of these columns is populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_excel('../data/Data_2020_P06/all_billings_inputs.xlsx', sheet_name='type_A_no_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.rename(index=str, columns={'Document Currency':'curr', \n",
    "                               'Enterprise BU Desc':'BU',\n",
    "                               'Invoice Fiscal Year Period Desc':'period',\n",
    "                               'Rev Rec Contract End Date Hdr':'end_date_1',\n",
    "                               'Rev Rec Contract End Date Item':'end_date_2',\n",
    "                               'Rev Rec Contract Start Date Hdr': 'start_date_1',\n",
    "                               'Rev Rec Contract Start Date Item': 'start_date_2',\n",
    "                               'Completed Sales ( DC )':'DC_amount',\n",
    "                               'Completed Sales': 'US_amount'\n",
    "                               }, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.head(20)\n",
    "#df_A.sample(5)\n",
    "#df_A.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing banned currencies\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_currencies(df, model_dict):\n",
    "    this_list = df['curr'].unique().tolist()\n",
    "    \n",
    "    for curr in model_dict['curr_removed']:\n",
    "        if curr in this_list:\n",
    "            print('need to ban this currency: ', curr)\n",
    "            df = df[df['curr']!= curr]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = remove_bad_currencies(df_A, model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Handling the duplicate dates by taking a max and creating a start_date and end_date fields in pandas datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A['start_date_str'] = df_A[['start_date_1','start_date_2']].max(axis=1).astype(str)\n",
    "df_A['end_date_str'] = df_A[['end_date_1','end_date_2']].max(axis=1).astype(str)\n",
    "\n",
    "df_A['start_date'] = pd.to_datetime(df_A['start_date_str'])\n",
    "df_A['end_date'] = pd.to_datetime(df_A['end_date_str'])\n",
    "\n",
    "df_A.drop(labels=['end_date_1', 'end_date_2', 'start_date_1', 'start_date_2',\n",
    "                  'start_date_str', 'end_date_str'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a month_interval field that calculates the difference between the start_date and end_date in months. We will map this number of months into a rebilling frequency (this number of months determines when the contract expires and the deferred revenue model assumes that all attribution is accounted for in our net new billings estimates provided by FP&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A['month_interval']=(df_A['end_date']-df_A['start_date'])\n",
    "df_A['months']= (df_A['month_interval']/ np.timedelta64(1,'M')).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.head(10)\n",
    "#df_A.sample(10)\n",
    "#df_A.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapping the number of months into our common rebill frequencies (monthly, quarterly, semi-annual, annual, 2 years and 3 years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rebills = [1, 3, 6, 12, 24, 36]\n",
    "temp_rebill = np.zeros_like(df_A['months'])\n",
    "for i in range(len(df_A)):\n",
    "    temp_rebill[i] = min(list_rebills, key=lambda x:abs(x-df_A['months'][i]))\n",
    "df_A['rebill_months']=temp_rebill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1, figsize=(14,6))\n",
    "axs.scatter(df_A['months'], df_A['rebill_months'])\n",
    "axs.set_ylabel('Rebill Months')\n",
    "axs.set_xlabel('Number of months between contract start and end dates')\n",
    "axs.set_title('Type A billings with no config type rebilling mapping')\n",
    "print_text = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.head(10)\n",
    "#df_A.sample(10)\n",
    "#df_A.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dropping the columns we no longer need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.drop(columns = ['start_date', 'end_date', 'month_interval', 'months'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grouping the dataframe by rebill_months using a pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#medals = df.pivot_table('no of medals', ['Year', 'Country'], 'medal')\n",
    "temp_DC = df_A.pivot_table('DC_amount', ['curr', 'BU', 'period'], 'rebill_months')\n",
    "temp_US = df_A.pivot_table('US_amount', ['curr', 'BU', 'period'], 'rebill_months')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Filling in any zeros that arise if there is no contract on a specific period, currency and BU for a particular rebill period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_DC = temp_DC.fillna(0)\n",
    "temp_US = temp_DC.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Flattening the pivot table back to a normal dataframe and renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_flat_DC = pd.DataFrame(temp_DC.to_records())\n",
    "temp_flat_US = pd.DataFrame(temp_US.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_flat_DC.rename(index=str, columns={'1.0':'deferred_1M_DC', \n",
    "                               '3.0':'deferred_3M_DC',\n",
    "                               '6.0':'deferred_6M_DC',\n",
    "                               '12.0':'deferred_1Y_DC',\n",
    "                               '24.0':'deferred_2Y_DC',\n",
    "                               '36.0': 'deferred_3Y_DC'}, inplace=True)\n",
    "\n",
    "temp_flat_US.rename(index=str, columns={'1.0':'deferred_1M_US', \n",
    "                               '3.0':'deferred_3M_US',\n",
    "                               '6.0':'deferred_6M_US',\n",
    "                               '12.0':'deferred_1Y_US',\n",
    "                               '24.0':'deferred_2Y_US',\n",
    "                               '36.0': 'deferred_3Y_US'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_flat_DC.head(20)\n",
    "#temp_flat_US.sample(20)\n",
    "#temp_flat_DC.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Quick check that we have not created duplicate column entries (for example two entries for a period with same BU and currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dup = df.copy()\n",
    "orig_len = len(df_test_dup)\n",
    "print(\"Original Length of the dataframe before duplicate test: \", orig_len)\n",
    "\n",
    "df_test_dup =df_test_dup.drop_duplicates(subset=['curr', 'BU', 'period'])\n",
    "print('New length of database after duplicates have been removed: ',len(df_test_dup))\n",
    "\n",
    "if orig_len!=len(df_test_dup):\n",
    "    print('We had duplicates in the dataframe! Look into why')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Merging the billings dataframe with the temp_flat_DC dataframe and and temp_flat_US dataframe and filling in any blanks with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_A = pd.merge(df, temp_flat_DC, how='outer',\n",
    "                    left_on= ['curr', 'BU', 'period'],\n",
    "                    right_on=['curr', 'BU', 'period'], indicator=True, validate='one_to_one')\n",
    "\n",
    "df_with_A = df_with_A.fillna(pd.Series(0, index=df_with_A.select_dtypes(exclude='category').columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_all = pd.merge(df_with_A, temp_flat_US, how='outer',\n",
    "                    left_on= ['curr', 'BU', 'period'],\n",
    "                    right_on=['curr', 'BU', 'period'])\n",
    "\n",
    "df_with_all = df_with_all.fillna(pd.Series(0, index=df_with_all.select_dtypes(exclude='category').columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_all.head(10)\n",
    "#df_with_all.sample(10)\n",
    "#df_with_all.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combining columns form the different data sources (they get merged with different names) and cleaning up the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_all['deferred_1M_DC']= df_with_all['deferred_1M_DC_x']+df_with_all['deferred_1M_DC_y']\n",
    "df_with_all['deferred_3M_DC']= df_with_all['deferred_3M_DC_x']+df_with_all['deferred_3M_DC_y']\n",
    "df_with_all['deferred_6M_DC']= df_with_all['deferred_6M_DC_x']+df_with_all['deferred_6M_DC_y']\n",
    "df_with_all['deferred_1Y_DC']= df_with_all['deferred_1Y_DC_x']+df_with_all['deferred_1Y_DC_y']\n",
    "df_with_all['deferred_2Y_DC']= df_with_all['deferred_2Y_DC_x']+df_with_all['deferred_2Y_DC_y']\n",
    "df_with_all['deferred_3Y_DC']= df_with_all['deferred_3Y_DC_x']+df_with_all['deferred_3Y_DC_y']\n",
    "\n",
    "df_with_all['deferred_1M_US']= df_with_all['deferred_1M_US_x']+df_with_all['deferred_1M_US_y']\n",
    "df_with_all['deferred_3M_US']= df_with_all['deferred_3M_US_x']+df_with_all['deferred_3M_US_y']\n",
    "df_with_all['deferred_6M_US']= df_with_all['deferred_6M_US_x']+df_with_all['deferred_6M_US_y']\n",
    "df_with_all['deferred_1Y_US']= df_with_all['deferred_1Y_US_x']+df_with_all['deferred_1Y_US_y']\n",
    "df_with_all['deferred_2Y_US']= df_with_all['deferred_2Y_US_x']+df_with_all['deferred_2Y_US_y']\n",
    "df_with_all['deferred_3Y_US']= df_with_all['deferred_3Y_US_x']+df_with_all['deferred_3Y_US_y']\n",
    "\n",
    "df_with_all.drop(labels = ['deferred_1M_DC_x','deferred_1M_DC_y',\n",
    "                        'deferred_3M_DC_x','deferred_3M_DC_y',\n",
    "                        'deferred_6M_DC_x','deferred_6M_DC_y',\n",
    "                        'deferred_1Y_DC_x','deferred_1Y_DC_y',\n",
    "                        'deferred_2Y_DC_x','deferred_2Y_DC_y',\n",
    "                        'deferred_3Y_DC_x','deferred_3Y_DC_y',\n",
    "                        'deferred_1M_US_x','deferred_1M_US_y',   \n",
    "                        'deferred_3M_US_x','deferred_3M_US_y',\n",
    "                        'deferred_6M_US_x','deferred_6M_US_y',\n",
    "                        'deferred_1Y_US_x','deferred_1Y_US_y',\n",
    "                        'deferred_2Y_US_x','deferred_2Y_US_y',\n",
    "                        'deferred_3Y_US_x','deferred_3Y_US_y'],\n",
    "                         axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_all.head(5)\n",
    "#df_with_all.sample(5)\n",
    "#df_with_all.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking totals to se if they match what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum of temp flat DC 1M:      ', temp_flat_DC['deferred_1M_DC'].sum())\n",
    "print('sum of base_df before DC 1M: ', df['deferred_1M_DC'].sum())\n",
    "print('sum of final DC 1M:          ', df_with_all['deferred_1M_DC'].sum())\n",
    "\n",
    "a = temp_flat_DC['deferred_1M_DC'].sum()\n",
    "b = df['deferred_1M_DC'].sum()\n",
    "c = df_with_all['deferred_1M_DC'].sum()\n",
    "print(c)\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE DONE: Create a table that contains the total billings by DC for each dataframe and each step for auditing\n",
    "\n",
    " - start with all of the DC\n",
    " - then create function that appends and adds rows\n",
    " - then do the same for the DC stuff type_A\n",
    " - then check the totals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming the cleaned billings dataframe as df_billings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings = df_with_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_billings['period'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking that there are no bilings from future periods in this dataframe. If so, drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_index= df_billings[df_billings['period']=='2020-07'].index\n",
    "#df_billings.drop(drop_index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorting the dataframe and saving this dataframe for use later in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings = df_billings.sort_values(['curr', 'BU', 'period'], ascending = (True, True, True))\n",
    "\n",
    "with open('../data/processed/all_billings.p', 'wb') as f:\n",
    "    pickle.dump(df_billings, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading All of the other information we need here from excel files\n",
    " - currency_map: contain a mapping of currency the majority of our billings in each country\n",
    " - FX_data: contains current spot rates, FX forward rates and FX volatilities\n",
    " - FX_forward_rates: contains the forward rates used in the FP&A Plan\n",
    " - Bookings Forecast: contains the most recent FP&A net new booking forecast (usually only one fiscal year included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Currency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curr_map = pd.read_excel(\"../data/Data_2020_P06/currency_map.xlsx\", sheet_name=\"curr_map\")\n",
    "df_curr_map[\"Country\"] = df_curr_map[\"Country\"].str.replace(\"\\(MA\\)\", \"\", case=False)\n",
    "df_curr_map['Country'] = df_curr_map['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_rates = pd.read_excel('../data/Data_2020_P06/FX_data.xlsx', sheet_name='to_matlab')\n",
    "df_FX_rates['VOL_3M'] = df_FX_rates['VOL_3M']/100\n",
    "df_FX_rates['VOL_6M'] = df_FX_rates['VOL_6M']/100\n",
    "df_FX_rates['VOL_9M'] = df_FX_rates['VOL_9M']/100\n",
    "df_FX_rates['VOL_1Y'] = df_FX_rates['VOL_1Y']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_rates.head(5)\n",
    "#df_FX_rates.sample(5)\n",
    "#df_FX_rates.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FX Forward Rates used in the FP&A Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_fwds = pd.read_excel('../data/Data_2020_P06/FX_forward_rates.xlsx', sheet_name='forward_data', \n",
    "                          skiprows = 1, usecols=\"C,G\")\n",
    "\n",
    "df_FX_fwds.rename(index=str, columns={'Unnamed: 2': 'curr', 'FWD REF':'forward'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the # below to see the entire list of FX_fwds in the plan\n",
    "df_FX_fwds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bookings Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings = pd.read_excel('../data/Data_2020_P06/2020_bookings_fcst_Q2.xlsx', sheet_name='source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings.head(10)\n",
    "#df_bookings.sample(10)\n",
    "#df_bookings.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the bookings data\n",
    "##### NOTE: The bookings spreadsheet looks very different for Q2 versus prior quarters!\n",
    " - remove odd strings such as '(GP)' from BU, (IS) from Internal Segment, etc\n",
    " - dropping columns we do not need\n",
    " - renaming columns to better match our data naming convention\n",
    " \n",
    " NOTE: The '('  and ')' is a special character so we need to precede these with the escape character '\\'\n",
    " \n",
    " NOTE: 2 The columns also have leading or trailing spaces, we need to strip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['EBU'] = df_bookings['EBU'].str.replace(' \\(GP\\)', '', case=False)\n",
    "df_bookings['Internal Segment'] = df_bookings['Internal Segment'].str.replace('\\(IS\\)', '')\n",
    "df_bookings['PMBU'] = df_bookings['PMBU'].str.replace('\\(PMBU\\)', '')\n",
    "df_bookings['Geo'] = df_bookings['Geo'].str.replace('\\(G\\)', '')\n",
    "df_bookings['Market Area'] = df_bookings['Market Area'].str.replace('\\(MA\\)', '')\n",
    "df_bookings['Booking Type (Low)'] = df_bookings['Booking Type (Low)'].str.replace('\\(MA\\)', '')\n",
    "\n",
    "df_bookings['EBU'] = df_bookings['EBU'].str.strip()\n",
    "df_bookings['Internal Segment'] = df_bookings['Internal Segment'].str.strip()\n",
    "df_bookings['PMBU'] = df_bookings['PMBU'].str.strip()\n",
    "df_bookings['Geo'] = df_bookings['Geo'].str.strip()\n",
    "df_bookings['Market Area'] = df_bookings['Market Area'].str.strip()\n",
    "df_bookings['Booking Type (Low)'] = df_bookings['Booking Type (Low)'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings.drop(columns = ['Bookings Hedge', 'Market Segment', 'Booking Type (High)', 'Plan', 'FX Conversion'], inplace = True)\n",
    "\n",
    "df_bookings.rename(index=str, columns = {'EBU': 'BU', \n",
    "                                        'Internal Segment': 'segment',\n",
    "                                        'PMBU': 'product',\n",
    "                                        'Geo':'geo',\n",
    "                                        'Market Area': 'country',\n",
    "                                        'Booking Type (Low)': 'booking_type',\n",
    "                                        'Value': 'US_amount'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings.head(10)\n",
    "#df_bookings.sample(10)\n",
    "#df_bookings.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are new BUs now! What the hell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot table Karen is using only look at 4 EBUs\n",
    " - Creative\n",
    " - Document Cloud\n",
    " - Digital Experience\n",
    " - Print & Publishing\n",
    " \n",
    " The following bookings types are used\n",
    " - ASV\n",
    " - Total Subscription Attrition\n",
    " - Consulting\n",
    " \n",
    " That is all that is in there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The cell below shows samples of what is in the data. Removing one of the parenthesis will execute the code. (One at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['BU'].value_counts()\n",
    "#df_bookings['segment'].value_counts()\n",
    "#df_bookings['product'].value_counts()\n",
    "#df_bookings['country'].value_counts()\n",
    "#df_bookings['booking_type'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_list = ['Data & Insights', \n",
    "              'Customer Journey Management',\n",
    "              'Commerce',\n",
    "              'Content',\n",
    "              'Shared Marketing Cloud',\n",
    "              'AEM Other',\n",
    "              'Adobe  Video Solutions']\n",
    "\n",
    "len(change_list)\n",
    "\n",
    "new_BU = ['Experience Cloud']\n",
    "new_BU_list = new_BU * len(change_list)\n",
    "\n",
    "change_dict = dict(zip(change_list, new_BU_list))\n",
    "print(change_dict)\n",
    "\n",
    "#In [127]: state_df = state_df.replace({\"state\": replace_values})     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['BU'] = df_bookings['BU'].replace(change_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['BU'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bookings['BU'].value_counts()\n",
    "#df_bookings['segment'].value_counts()\n",
    "#df_bookings['product'].value_counts()\n",
    "df_bookings['country'].value_counts()\n",
    "#df_bookings['booking_type'].value_counts();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The countries now contain two fields that we need to change\n",
    "- UNKNOWN\n",
    "- AMER #\n",
    "\n",
    "These will be changed to United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['country'] = df_bookings['country'].replace({'AMER #': 'United States',\n",
    "                                                        'UNKNOWN': 'United States'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings['booking_type'].value_counts()\n",
    "#df_bookings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "### Merging the bookings country data to a currency using the currency map dataframe (df_curr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_book_ctry = df_bookings['country'].unique()\n",
    "print('Countries in the bookings file: \\n', list_book_ctry)\n",
    "\n",
    "list_curr_map = df_curr_map['Country'].unique()\n",
    "print('Countries in the currency map file: \\n', list_curr_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking that we have the currency mapping for every country where we have a bookings forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(set(list_book_ctry) & set(list_curr_map))\n",
    "\n",
    "not_in_map = set(list_book_ctry).difference(set(list_curr_map))\n",
    "if len(not_in_map)!=0:\n",
    "    print('There is a bookings currency that is not in the currency map!\\nWe need to look into the currency map file and add this!')\n",
    "else:\n",
    "    print('The bookings currencies are in the currency map. OK to merge the dataframes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Merge the bookings forecast with the currency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings = pd.merge(df_bookings, df_curr_map, how='left', left_on='country', right_on='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bookings.head(10)\n",
    "df_bookings.sample(10)\n",
    "#df_bookings.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding periods weeks (from the Adobe calendar) to the billings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cal.head(10)\n",
    "#df_cal.sample(10)\n",
    "#df_cal.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a column in df_cal with year  '-' the last two digits of the per_ticker to match with the billings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal['p2digit']=df_cal['Period'].astype(str)\n",
    "df_cal['p2digit']=df_cal['p2digit'].str.zfill(2)\n",
    "\n",
    "df_cal['period_match']=df_cal['Year'].astype(str) + '-' + df_cal['p2digit'].astype(str)\n",
    "\n",
    "df_cal.drop(['p2digit'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cal.head(10)\n",
    "df_cal.sample(10)\n",
    "#df_cal.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting the calendar ready to be merged with the df_billings dataframe by removing columns that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal_2_merge = df_cal.copy()\n",
    "df_cal_2_merge.drop(['Year', 'Quarter', 'Period', 'Qtr_Ticker', 'Qtr_Start', 'Qtr_End', 'Per_Start',\n",
    "                     'Per_Ticker','Per_End'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging the calendar periods with the periods in the df_billings dataframe to bring over period weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings = df_billings.merge(df_cal_2_merge, how='left', left_on='period', right_on='period_match')\n",
    "df_billings.drop(['period_match', '_merge'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings.head(5)\n",
    "#df_billings.sample(5)\n",
    "#df_billings.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving these dataframes in as a python dictionary in the pickle file 'all_inputs.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings=df_billings.sort_values(['curr', 'BU', 'period'], ascending = (True, True, True))\n",
    "\n",
    "input_df_dict = {'model_dict': model_dict,\n",
    "                 'billings':df_billings, \n",
    "                 'ADBE_cal':df_cal,\n",
    "                 'bookings': df_bookings,\n",
    "                 'FX_forwards': df_FX_fwds,\n",
    "                 'FX_rates': df_FX_rates\n",
    "                }\n",
    "\n",
    "pickle.dump(input_df_dict, open('../data/processed/all_inputs.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the billings dataframe\n",
    "- the billings dataframe does not contain every period if there are no bookings within a period.\n",
    "- the easiest way to create the forecast requires that we have all of the periods in each BU and currency pair (or at least 36 months worth so that we can incorporate the 3 year deferred bookings\n",
    "\n",
    "###### The bookings foreacast also contains products such as 'LiveCycle' and 'other solutions' that we do not expect to recieve billings for going forward (there are no booking associated with this) so we need to remove them from the billings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_billings_periods(df_billings):\n",
    "    # clean up billings by removing LiveCycle and other solutions\n",
    "    index_lc = df_billings[df_billings['BU']=='LiveCycle'].index\n",
    "    df_billings.drop(index_lc, inplace=True)\n",
    "\n",
    "    index_other = df_billings[df_billings['BU']=='Other Solutions'].index\n",
    "    df_billings.drop(index_other, inplace=True)\n",
    "\n",
    "    \n",
    "    all_BU = df_billings['BU'].unique()\n",
    "    all_curr = df_billings['curr'].unique()\n",
    "\n",
    "    all_periods = df_billings['period'].unique()\n",
    "    all_periods = np.sort(all_periods)\n",
    "    all_periods = all_periods[-36:]\n",
    "\n",
    "\n",
    "    list_new_BUs = []\n",
    "    list_new_currs = []\n",
    "    list_new_periods = []\n",
    "\n",
    "    for this_BU in all_BU:\n",
    "\n",
    "        for this_curr in all_curr:\n",
    "\n",
    "            df_slice = df_billings[(df_billings['BU']== this_BU)&\n",
    "                                   (df_billings['curr']==this_curr)].copy()\n",
    "\n",
    "            list_periods = df_slice['period'].unique()\n",
    "            set_periods = set(list_periods)\n",
    "            set_all = set(all_periods)\n",
    "\n",
    "            periods_missing = set_all.difference(set_periods)\n",
    "\n",
    "            for i in periods_missing:\n",
    "                list_new_periods.append(i)\n",
    "                list_new_currs.append(this_curr)\n",
    "                list_new_BUs.append(this_BU)\n",
    "\n",
    "\n",
    "    df_to_add = pd.DataFrame({'curr': list_new_currs, \n",
    "                              'BU': list_new_BUs,\n",
    "                              'period': list_new_periods})\n",
    "\n",
    "    df_billings_check = pd.concat([df_billings, df_to_add], sort=False)\n",
    "\n",
    "    df_billings_check = df_billings_check.fillna(0)\n",
    "\n",
    "    df_billings = df_billings_check.copy()\n",
    "    \n",
    "    df_billings=df_billings.sort_values(['curr', 'BU', 'period'], ascending = (True, True, True))\n",
    "\n",
    "    return df_billings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explicit call to the add_billings_periods function is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of df_billings before removal of old BUs and adding periods:', len(df_billings))\n",
    "df_billings = add_billings_periods(df_billings)\n",
    "print('Length of df_billings after removal of old BUs and adding periods:', len(df_billings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the bookings dataframe to be incorporated into the deferred model\n",
    "- The billings dataframe is by period\n",
    "- the bookings dataframe contains net new bookings by quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the last period in the billings index\n",
    "last_period = '2020-06'\n",
    "\n",
    "list_BUs = df_bookings['BU'].unique()\n",
    "list_curr = df_bookings['Currency'].unique()\n",
    "\n",
    "print('This is the list of BUs in the bookings dataframe: ', list_BUs)\n",
    "print('This is the list of currencies in the bookings dataframe: ', list_curr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating data to add to the billings dataframe to incorporate period by period billings \n",
    "NOTE:  This is just creating the space in the dataframe for the data. We will fill it in later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe of zeros\n",
    "l_BU = []\n",
    "l_curr = []\n",
    "for BU in list_BUs:\n",
    "    for curr in list_curr:\n",
    "        l_BU.append(BU)\n",
    "        l_curr.append(curr)\n",
    "#print(l_BU)\n",
    "#print(l_curr)\n",
    "l_zero = np.zeros(len(l_BU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= {'BU':l_BU, 'curr':l_curr, \n",
    "      'Q1':l_zero,\n",
    "      'Q2':l_zero,\n",
    "      'Q3':l_zero,\n",
    "      'Q4':l_zero,\n",
    "      'P01':l_zero,\n",
    "      'P02':l_zero,\n",
    "      'P03':l_zero,\n",
    "      'P04':l_zero,\n",
    "      'P05':l_zero,\n",
    "      'P06':l_zero,\n",
    "      'P07':l_zero,\n",
    "       'P08':l_zero,\n",
    "       'P09':l_zero,\n",
    "       'P10':l_zero,\n",
    "       'P11':l_zero,\n",
    "       'P12':l_zero,\n",
    "      }\n",
    "\n",
    "df_book_period=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_book_period.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uncomment below to remember what the df_bookings looked like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings.head(10)\n",
    "#df_bookings.sample(10)\n",
    "#df_bookings.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below fills in the df_book_period dataframe with the quarterly bookings numbers for each BU and currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the quarters\n",
    "for i in range(len(df_book_period['BU'])):\n",
    "    \n",
    "    this_BU = df_book_period['BU'][i]\n",
    "    this_curr = df_book_period['curr'][i]\n",
    "    this_slice = df_bookings[(df_bookings['BU']==this_BU)&\n",
    "                          (df_bookings['Currency']==this_curr)]\n",
    "    \n",
    "    this_Q1= this_slice[this_slice['Quarter']=='Q1 2020']\n",
    "    sum_Q1 = this_Q1['US_amount'].sum()\n",
    "    df_book_period['Q1'].loc[i]=sum_Q1\n",
    "    \n",
    "    this_Q2= this_slice[this_slice['Quarter']=='Q2 2020']\n",
    "    sum_Q2 = this_Q2['US_amount'].sum()\n",
    "    df_book_period['Q2'].loc[i]=sum_Q2\n",
    "    \n",
    "    this_Q3= this_slice[this_slice['Quarter']=='Q4 2020']\n",
    "    sum_Q3 = this_Q3['US_amount'].sum()\n",
    "    df_book_period['Q3'].loc[i]=sum_Q3\n",
    "    \n",
    "    this_Q4= this_slice[this_slice['Quarter']=='Q4 2020']\n",
    "    sum_Q4 = this_Q4['US_amount'].sum()\n",
    "    df_book_period['Q4'].loc[i]=sum_Q4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_period.head(20)\n",
    "#df_book_period.sample(10)\n",
    "#df_book_period.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating lists of periods and quarters needed to fill out the df_book_period dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of quarters for the percentages\n",
    "list_q2 = ['2019-04', '2019-05', '2019-06']\n",
    "list_q3 = ['2019-07', '2019-08', '2019-09']\n",
    "list_q4 = ['2019-10', '2019-11', '2019-12']\n",
    "list_q1 = [ '2020-01', '2020-02', '2020-03']\n",
    "\n",
    "list_periods = ['2020-01', '2020-02', '2020-03',\n",
    "                '2019-04', '2019-05', '2019-06',\n",
    "                '2019-07', '2019-08', '2019-09',\n",
    "                '2019-10', '2019-11', '2019-12']\n",
    "\n",
    "list_p_headers = ['P01', 'P02', 'P03',\n",
    "                  'P04', 'P05', 'P06',\n",
    "                  'P07', 'P08', 'P09',\n",
    "                  'P10', 'P11', 'P12'\n",
    "                 ]\n",
    "\n",
    "list_q_headers = ['Q1', 'Q1', 'Q1',\n",
    "                  'Q2', 'Q2', 'Q2',\n",
    "                  'Q3', 'Q3', 'Q3',\n",
    "                  'Q4', 'Q4', 'Q4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adding the booking periods to the dataframe. The bookings are split into periods based on last years percentage of 1 year deferred billings within the quarter.\n",
    "For example: P1 = 25%, P2 = 30%, P3 = 45% such that the sum is equal to the total quarterly billings last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_book_period['BU'])):\n",
    "    \n",
    "    this_BU = df_book_period['BU'][i]\n",
    "    this_curr = df_book_period['curr'][i]\n",
    "    \n",
    "    this_slice = df_billings[(df_billings['BU']==this_BU)&\n",
    "                          (df_billings['curr']==this_curr)]\n",
    "    \n",
    "    for j in range(len(list_periods)):\n",
    "        this_period = list_periods[j]\n",
    "        this_header = list_p_headers[j]\n",
    "        this_quarter = list_q_headers[j]\n",
    "        this_P_slice = this_slice[this_slice['period']==this_period]\n",
    "        df_book_period.loc[[i],[this_header]]=this_P_slice['deferred_1Y_DC'].sum()\n",
    "        \n",
    "df_book_period['bill_Q1_sum'] = df_book_period['P01'] + df_book_period['P02'] + df_book_period['P03']    \n",
    "df_book_period['bill_Q2_sum'] = df_book_period['P04'] + df_book_period['P05'] + df_book_period['P06']    \n",
    "df_book_period['bill_Q3_sum'] = df_book_period['P07'] + df_book_period['P08'] + df_book_period['P09']    \n",
    "df_book_period['bill_Q4_sum'] = df_book_period['P10'] + df_book_period['P11'] + df_book_period['P12']    \n",
    "\n",
    "df_book_period['P01'] = df_book_period['Q1']*df_book_period['P01']/df_book_period['bill_Q1_sum']\n",
    "df_book_period['P02'] = df_book_period['Q1']*df_book_period['P02']/df_book_period['bill_Q1_sum']\n",
    "df_book_period['P03'] = df_book_period['Q1']*df_book_period['P03']/df_book_period['bill_Q1_sum']\n",
    "\n",
    "df_book_period['P04'] = df_book_period['Q2']*df_book_period['P04']/df_book_period['bill_Q2_sum']\n",
    "df_book_period['P05'] = df_book_period['Q2']*df_book_period['P05']/df_book_period['bill_Q2_sum']\n",
    "df_book_period['P06'] = df_book_period['Q2']*df_book_period['P06']/df_book_period['bill_Q2_sum']\n",
    "\n",
    "df_book_period['P07'] = df_book_period['Q3']*df_book_period['P07']/df_book_period['bill_Q3_sum']\n",
    "df_book_period['P08'] = df_book_period['Q3']*df_book_period['P08']/df_book_period['bill_Q3_sum']\n",
    "df_book_period['P09'] = df_book_period['Q3']*df_book_period['P09']/df_book_period['bill_Q3_sum']\n",
    "\n",
    "df_book_period['P10'] = df_book_period['Q4']*df_book_period['P10']/df_book_period['bill_Q4_sum']\n",
    "df_book_period['P11'] = df_book_period['Q4']*df_book_period['P11']/df_book_period['bill_Q4_sum']\n",
    "df_book_period['P12'] = df_book_period['Q4']*df_book_period['P12']/df_book_period['bill_Q4_sum']\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_book_period.head(10)\n",
    "#df_book_period.sample(10)\n",
    "df_book_period.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaning up the dataframe by dropping the columns we no longer need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_period.drop(['bill_Q1_sum', 'bill_Q2_sum', 'bill_Q3_sum', 'bill_Q4_sum'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_period.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting these billings to local currency based on the forward rates at the time the plan was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_fwds.set_index('curr', inplace=True)\n",
    "\n",
    "list_fwds =[]\n",
    "for i in range(len(df_book_period['curr'])):\n",
    "    this_curr = df_book_period['curr'][i]\n",
    "    \n",
    "    if this_curr == 'USD':\n",
    "        this_fwd=1\n",
    "    else:\n",
    "        this_fwd = df_FX_fwds.loc[this_curr, 'forward']\n",
    "    \n",
    "    \n",
    "    list_fwds.append(this_fwd)\n",
    "df_book_period['FX_fwd_rate'] = list_fwds\n",
    "\n",
    "df_book_period['P01_US']=df_book_period['P01']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P02_US']=df_book_period['P02']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P03_US']=df_book_period['P03']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P04_US']=df_book_period['P04']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P05_US']=df_book_period['P05']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P06_US']=df_book_period['P06']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P07_US']=df_book_period['P07']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P08_US']=df_book_period['P08']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P09_US']=df_book_period['P09']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P10_US']=df_book_period['P10']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P11_US']=df_book_period['P11']* df_book_period['FX_fwd_rate']\n",
    "df_book_period['P12_US']=df_book_period['P12']* df_book_period['FX_fwd_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_book_period.head(10)\n",
    "#df_book_period.sample(10)\n",
    "df_book_period.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The df_book_period dataframe now has columns for bookings each period in both local currency and document currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_period.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the billings forecast in a dataframe called df_fcst\n",
    "\n",
    "###  Forecasting the billings into the future\n",
    "#### Steps\n",
    " - create list of bill periods that is sorted for the lookup functions\n",
    " - create forecast dataframe that includes the same columns (though in document currency) for the billings\n",
    " - add the bookings forecast to this data\n",
    " - create impact on deferred (project the new waterfall from this_\n",
    " - load up accounting's version of the initial waterfall (by BU)\n",
    " - reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### creating the list of historical bill periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bill_periods = df_billings['period'].unique()\n",
    "list_bill_periods.sort()\n",
    "print(list_bill_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_BU = df_billings['BU'].copy()\n",
    "v_curr = df_billings['curr'].copy()\n",
    "v_both = v_BU + v_curr\n",
    "v_unique = v_both.unique()\n",
    "\n",
    "v_un_BU = [sub[:-3] for sub in v_unique]\n",
    "v_un_curr = [sub[-3:] for sub in v_unique]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_future_periods = ['2020-07', '2020-08', '2020-09',\n",
    "                       '2020-10', '2020-11', '2020-12',\n",
    "                       '2021-01', '2021-02', '2021-03',\n",
    "                       '2021-04', '2021-05', '2021-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vectors for the future billings dataframe\n",
    "v_BU_2_df=[]\n",
    "v_curr_2_df=[]\n",
    "v_period_2_df = []\n",
    "\n",
    "for i in range(len(v_un_BU)):\n",
    "    this_BU = v_un_BU[i]\n",
    "    this_curr = v_un_curr[i]\n",
    "    \n",
    "    for period in list_future_periods:\n",
    "        v_BU_2_df.append(this_BU)\n",
    "        v_curr_2_df.append(this_curr)\n",
    "        v_period_2_df.append(period)\n",
    "\n",
    "print('This is the length of the vectors: ',len(v_BU_2_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a list of the columns that we need to use in the df_billings dataframe (They contain document currency billings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_columns = df_billings.columns\n",
    "\n",
    "list_keepers= []\n",
    "for i in list_all_columns:\n",
    "    \n",
    "    if i[-2:]=='DC':\n",
    "        list_keepers.append(i)\n",
    "\n",
    "list_keepers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the df_fcst dataframe with every currency, BU and period we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst = pd.DataFrame({'curr': v_curr_2_df,\n",
    "                        'BU': v_BU_2_df,\n",
    "                       'period': v_period_2_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adding the columns we need to populate (list_keepers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list_keepers:\n",
    "    df_fcst[col]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.head(10)\n",
    "#df_fcst.sample(10)\n",
    "#df_fcst.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding period weeks to the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal_2_merge = df_cal.copy()\n",
    "df_cal_2_merge.drop(['Year', 'Quarter', 'Period', 'Qtr_Ticker', 'Qtr_Start', 'Qtr_End', 'Per_Start',\n",
    "                     'Per_Ticker','Per_End'], axis=1, inplace=True)\n",
    "\n",
    "df_fcst = df_fcst.merge(df_cal_2_merge, how='left', left_on='period', right_on='period_match')\n",
    "df_fcst.drop(['period_match'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.head(10)\n",
    "#df_fcst.sample(10)\n",
    "#df_fcst.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The functions below create the billings forecast by looking up the historical billings and having them renew\n",
    "NOTE: The monthly billings are using a linear regression model on the monthly billings / weeks in the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO BE DONE TO COMPLETE:\n",
    " - the monthly billings contain several BU, currency pairs that have no monthly billings history. We need to shortcut the program by adding an if statement in that case\n",
    " - we need to alter the monthly program to search the periods for the best time period to use (maximizing the R-squared) since some of the BU, currency pairs exhibit growth only after a few years\n",
    " - determine which print statement need to be kept to make sure it is running appropriately\n",
    " - remove slice error warnings after investigating where the problem occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_curr_and_BU(df_billings):\n",
    "    v_BU = df_billings['BU'].copy()\n",
    "    v_curr = df_billings['curr'].copy()\n",
    "    v_both = v_BU + v_curr\n",
    "    v_unique = v_both.unique()\n",
    "\n",
    "    v_un_BU = [sub[:-3] for sub in v_unique]\n",
    "    v_un_curr = [sub[-3:] for sub in v_unique]\n",
    "\n",
    "    return v_un_BU, v_un_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_billing_forecast(df_billings, df_fcst):\n",
    "\n",
    "    v_un_BU, v_un_curr = find_unique_curr_and_BU(df_billings)\n",
    "    \n",
    "    # new Vectorized approach (sort of)\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(v_un_BU)):\n",
    "        this_BU = v_un_BU[i]\n",
    "        this_curr = v_un_curr[i]\n",
    "        \n",
    "        print('working on BU: {0}  and currency: {1}'.format(this_BU, this_curr))\n",
    "        df_slice = df_billings[(df_billings['BU']==this_BU) &\n",
    "                                (df_billings['curr']== this_curr)].copy()\n",
    "\n",
    "\n",
    "        old_per_3Y = list_bill_periods[-36:-24]\n",
    "        old_per_2Y = list_bill_periods[-24:-12]\n",
    "        old_per_1Y = list_bill_periods[-12:]\n",
    "        old_per_6M = list_bill_periods[-6:]\n",
    "        old_per_3M = list_bill_periods[-3:]\n",
    "\n",
    "        # three year\n",
    "        this_v_3yrs = df_slice.loc[df_slice['period'].isin(old_per_3Y), 'deferred_3Y_DC'].copy()\n",
    "        if len(this_v_3yrs)!=12:\n",
    "            print(this_BU, this_curr)\n",
    "            print(\"There is a period mismatch. length of 3yrs vector = \", len(this_v_3yrs))\n",
    "            print('Length of df_slice: ', len(df_slice))    \n",
    "            print('This BU: {0} and this currency: {1}'.format(this_BU, this_curr))\n",
    "\n",
    "        else:        \n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                            (df_fcst['curr']==this_curr),\n",
    "                            'deferred_3Y_DC'] = this_v_3yrs.values\n",
    "\n",
    "        #two years\n",
    "        this_v_2yrs = df_slice.loc[df_slice['period'].isin(old_per_2Y), 'deferred_2Y_DC'].copy()\n",
    "        if len(this_v_2yrs)!=12:\n",
    "            print(this_BU, this_curr)\n",
    "            print(\"There is a period mismatch. length of 2 yrs vector = \", len(this_v_2yrs))\n",
    "            print('Length of df_slice: ', len(df_slice))    \n",
    "            print('This BU: {0} and this currency: {1}'.format(this_BU, this_curr))\n",
    "        else:\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                        (df_fcst['curr']==this_curr),\n",
    "                        'deferred_2Y_DC'] = this_v_2yrs.values\n",
    "\n",
    "        # one year\n",
    "        this_v_1yrs = df_slice.loc[df_slice['period'].isin(old_per_1Y), 'deferred_1Y_DC'].copy()\n",
    "        if len(this_v_1yrs)!= 12:\n",
    "            print(this_BU, this_curr)\n",
    "            print(\"There is a period mismatch. length of 1 yr vector = \", len(this_v_1yrs))\n",
    "            print('Length of df_slice: ', len(df_slice))    \n",
    "\n",
    "        else:\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                        (df_fcst['curr']==this_curr),\n",
    "                        'deferred_1Y_DC'] = this_v_1yrs.values\n",
    "\n",
    "        # six months (we need to append the values to repeat once)\n",
    "        this_v_6M = df_slice.loc[df_slice['period'].isin(old_per_6M), 'deferred_6M_DC'].copy()\n",
    "        this_v_6M = this_v_6M.append(this_v_6M, ignore_index=True)\n",
    "\n",
    "        df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                    (df_fcst['curr']==this_curr),\n",
    "                    'deferred_6M_DC'] = this_v_6M.values\n",
    "\n",
    "        # three months:\n",
    "        this_v_3M = df_slice.loc[df_slice['period'].isin(old_per_3M), 'deferred_3M_DC'].copy()\n",
    "        this_v_3M = this_v_3M.append(this_v_3M, ignore_index=True)\n",
    "        this_v_3M = this_v_3M.append(this_v_3M, ignore_index=True)\n",
    "\n",
    "        df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                    (df_fcst['curr']==this_curr),\n",
    "                    'deferred_3M_DC'] = this_v_3M.values\n",
    "\n",
    "        # what the hell do we do with the service and recognized revenue billings?\n",
    "        # RECOGNIZED REVENUE - does not go to deferred, so just take the last 12 month's worth\n",
    "        this_recog = df_slice.loc[df_slice['period'].isin(old_per_1Y), 'recognized_DC'].copy()\n",
    "        df_fcst.loc[(df_fcst['BU']==this_BU) &\n",
    "                    (df_fcst['curr']==this_curr),\n",
    "                   'recognized_DC'] = this_recog.values\n",
    "\n",
    "        # SERVICE BASED BILLINGS - for now just use the average of whatever we used last time\n",
    "        this_svc = df_slice.loc[df_slice['period'].isin(old_per_1Y), 'service_DC'].copy()\n",
    "        df_fcst.loc[(df_fcst['BU']==this_BU) &\n",
    "                    (df_fcst['curr']==this_curr),\n",
    "                   'service_DC'] = this_svc.values\n",
    "\n",
    "        # Type B Deferred (Service Billings)\n",
    "        this_type_B = df_slice.loc[df_slice['period'].isin(old_per_1Y), 'deferred_B_DC'].copy()\n",
    "        df_fcst.loc[(df_fcst['BU']==this_BU) &\n",
    "                    (df_fcst['curr']==this_curr),\n",
    "                   'deferred_B_DC'] = this_type_B.values\n",
    "        \n",
    "        # MONTHLY BILLINGS\n",
    "        # here we need to call a seperate function using just the X array that is the one month billings\n",
    "        this_y= df_slice['deferred_1M_DC'].copy()\n",
    "        this_y = this_y.to_numpy()\n",
    "        this_y = this_y.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        if sum(this_y)!=0:\n",
    "        \n",
    "            period_weeks = df_slice['Period_Weeks'].copy()\n",
    "            period_weeks = period_weeks.to_numpy()\n",
    "            period_weeks = period_weeks.reshape(-1,1)\n",
    "\n",
    "            this_y = np.true_divide(this_y, period_weeks)\n",
    "            this_y = np.nan_to_num(this_y)\n",
    "            X = np.arange(len(this_y))\n",
    "\n",
    "            this_model  = build_monthly_forecast(X, this_y)\n",
    "            weekly_fcst_y = this_model['fcst_y'] \n",
    "            \n",
    "            fcst_slice = df_fcst[(df_fcst['BU']==this_BU)&\n",
    "                                 (df_fcst['curr']==this_curr)].copy()\n",
    "            fcst_weeks = fcst_slice['Period_Weeks'].to_numpy()\n",
    "            fcst_weeks=fcst_weeks.reshape(-1,1)\n",
    "\n",
    "            period_fcst_y = weekly_fcst_y * fcst_weeks\n",
    "            \n",
    "            #print('length of new_y: ', len(fcst_y))\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU) &\n",
    "                        (df_fcst['curr']==this_curr),\n",
    "                        'deferred_1M_DC'] = period_fcst_y\n",
    "\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                       (df_fcst['curr']==this_curr),\n",
    "                        'r_squared']= this_model['score']\n",
    "\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                       (df_fcst['curr']==this_curr),\n",
    "                        'intercept']= this_model['intercept']\n",
    "\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                       (df_fcst['curr']==this_curr),\n",
    "                        'coeff']= this_model['coeff']\n",
    "\n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                   (df_fcst['curr']==this_curr),\n",
    "                    'X_length']= this_model['first_row']\n",
    "        \n",
    "        #print('For this BU: {0} and this currency {1}, we have a score of {2}, and int of {3} and a coeff of {4}'.\n",
    "        #     format(this_BU, this_curr, this_score, this_int, this_coeff))\n",
    "        #NOTE: We will need to return two things here\n",
    "        # First - the df_fcst dataframe\n",
    "        # second - a dictionary describing the monthly forecasts\n",
    "    \n",
    "    \n",
    "    return df_fcst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_monthly_forecast(X, y):\n",
    "    '''\n",
    "    Need to keep track of the initial X and Y\n",
    "    Need to track the best X, best Y, best model & best score\n",
    "    Within the loop, reducing the X and Y to new_x and new_y and keeping track of new model\n",
    "    If the new model is better, the best_x, best_y, best_model and best_score are overwritten\n",
    "    \n",
    "    At the end of the program, the best model is fit and the relevant information is returned\n",
    "    '''\n",
    "    \n",
    "    X = X.reshape(-1,1)\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    best_X = X.reshape(-1,1)\n",
    "    best_y = y.reshape(-1,1)\n",
    "    \n",
    "    fcst_X = np.arange(np.max(X)+1, np.max(X)+13)\n",
    "    fcst_X = fcst_X.reshape(-1,1)\n",
    "    \n",
    "    # best row tracks the beginning month used for the model\n",
    "    best_row = 0\n",
    "    \n",
    "    # create initial linear regression model, fit it and record score\n",
    "    best_model = LinearRegression(fit_intercept=True)\n",
    "    best_model.fit(best_X, best_y)\n",
    "    best_score = best_model.score(best_X, best_y)\n",
    "    best_int =   best_model.intercept_\n",
    "    best_coeff = best_model.coef_\n",
    "\n",
    "    #print(\"Model Score :\",       best_score)\n",
    "    #print(\"Model intercept :\",   best_model.intercept_)\n",
    "    #print(\"Model Coefficient :\", best_model.coef_)\n",
    "\n",
    "    for start_row in np.arange(1, y.shape[0]-12):\n",
    "        new_X = X[start_row:]\n",
    "        new_y = y[start_row:]\n",
    "        \n",
    "        new_model = LinearRegression(fit_intercept=True)\n",
    "        new_model.fit(new_X, new_y)\n",
    "        new_score = new_model.score(new_X, new_y)\n",
    "        new_int =   new_model.intercept_\n",
    "        new_coeff = new_model.coef_\n",
    "        \n",
    "        #print(\"Model Score :\",       new_score)\n",
    "        #print(\"Model intercept :\",   new_model.intercept_)\n",
    "        #print(\"Model Coefficient :\", new_model.coef_)\n",
    "        \n",
    "\n",
    "        # if the new model beats the best model, reassign to the best model\n",
    "        if new_score > best_score:\n",
    "            best_model = new_model\n",
    "            best_score = new_score\n",
    "            best_X = new_X\n",
    "            best_y = new_y\n",
    "            best_row = start_row\n",
    "            best_int = new_int\n",
    "            best_coeff = new_coeff\n",
    "            \n",
    "    #perform the forecast\n",
    "    fcst_y = best_model.predict(fcst_X)\n",
    "    \n",
    "    \n",
    "    monthly_model = dict({'model':  best_model,\n",
    "                     'score':  best_score,\n",
    "                     'fcst_y': fcst_y,\n",
    "                     'first_row': best_row,\n",
    "                     'intercept': best_int,\n",
    "                     'coeff' : best_coeff\n",
    "                    })\n",
    "    \n",
    "        \n",
    "    return monthly_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst = create_billing_forecast(df_billings, df_fcst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.head(40)\n",
    "#df_fcst.sample(20)\n",
    "#df_fcst.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS WOULD BE A GREAT PLACE TO PUT AN INTERACTIVE CHART TO SEE WHAT IS GOING ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = df_fcst[(df_fcst['curr']=='EUR')&\n",
    "                     (df_fcst['BU']=='Creative')]\n",
    "test_output.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the initial work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings=df_billings.sort_values(['curr', 'BU', 'period'], ascending = (True, True, True))\n",
    "df_fcst = df_fcst.sort_values(['curr', 'BU', 'period'], ascending = (True, True, True))\n",
    "\n",
    "input_df_dict = {'model_dict': model_dict,\n",
    "                 'billings':df_billings, \n",
    "                 'ADBE_cal':df_cal,\n",
    "                 'bookings': df_bookings,\n",
    "                 'FX_forwards': df_FX_fwds,\n",
    "                 'FX_rates': df_FX_rates,\n",
    "                 'forecast': df_fcst\n",
    "                }\n",
    "\n",
    "pickle.dump(input_df_dict, open('../data/processed/initial_forecast.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating USD amounts for the forecast\n",
    "We have the document currency forecast of billings and now have to translate this into USD\n",
    " - df_fcst (contains the forecast and historical billings)\n",
    " - FX_rates (contains current spot rates, forward rates and volatilities)\n",
    " \n",
    " ##### I will need to create a 12 month forward vector for each currency\n",
    "  - First add 'is_direct' field to the df_FX_rates DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_FX_fwds(df_FX_rates):\n",
    "    ''' Creates monthly interpolated rates from the df_FX_rates file and adds the is_direct field '''\n",
    "    # Create list of tickers to determine which is direct (if USD is the first currency, it is direct)\n",
    "    tickers = df_FX_rates['Ticker'].copy()\n",
    "    first_curr = [sub[:-3] for sub in tickers]\n",
    "    is_direct = []\n",
    "    for curr in first_curr:\n",
    "        if curr=='USD':\n",
    "            is_direct.append(0)\n",
    "        else:\n",
    "            is_direct.append(1)\n",
    "\n",
    "    df_FX_rates['is_direct']=is_direct\n",
    "    \n",
    "    # Add new columns that will hold the forward rates\n",
    "    new_cols = ['fwd_01M', 'fwd_02M', 'fwd_03M',\n",
    "               'fwd_04M', 'fwd_05M', 'fwd_06M',\n",
    "               'fwd_07M', 'fwd_08M', 'fwd_09M',\n",
    "               'fwd_10M', 'fwd_11M', 'fwd_12M']\n",
    "\n",
    "    for item in new_cols:\n",
    "        df_FX_rates[item]=0\n",
    "        \n",
    "    # Interpolate the forward rates\n",
    "    interp_time = np.arange(1, 13)\n",
    "    interp_time = interp_time/12\n",
    "\n",
    "    fwd_times = [0, .25, .5, .75, 1]\n",
    "    \n",
    "    for index, row in df_FX_rates.iterrows():\n",
    "        fwds = [row['Spot'], row['FWD_3M'], row['FWD_6M'], row['FWD_9M'], row['FWD_1Y']]\n",
    "        interp_fwds = np.interp(interp_time, fwd_times, fwds)\n",
    "        for i in np.arange(len(new_cols)):\n",
    "        \n",
    "            df_FX_rates.loc[index, new_cols[i]]=interp_fwds[i]\n",
    "    \n",
    "    return df_FX_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_rates = interp_FX_fwds(df_FX_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_rates.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FX_rates.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating USD forecast\n",
    " - loop through the currencies and business units again\n",
    " - find the forward rates that need to be calculated, transpose and invert if is_direct = 1\n",
    " - take the time index and loop through the forwards to apply the forward rates to each DC amount\n",
    " - \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE DONE:\n",
    " - create the loop to iterate over each BU and currency (function is already written and used above)\n",
    " - multiply each of the following by the tranp_fwds to get the USD amounts\n",
    "     - recognized_DC\n",
    "     - service_DC\n",
    "     - deferred_B_DC\n",
    "     - deferred_1M_DC\n",
    "     - deferred_3M_DC\n",
    "     - deferred_6M_DC\n",
    "     - deferred_1Y_DC\n",
    "     - deferred_2Y_DC\n",
    "     - deferred_3Y_DC\n",
    " \n",
    " - make this function easy to use by tweaking the forward rates or shocking the FX_rates \n",
    "      - This will be a call to the df_FX_rates which will then recalculate the forward rates\n",
    "      - Then the new forward rates will need to be fed back into the df_fcst dataframe to recalculate USD amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['fwd_01M', 'fwd_02M', 'fwd_03M',\n",
    "               'fwd_04M', 'fwd_05M', 'fwd_06M',\n",
    "               'fwd_07M', 'fwd_08M', 'fwd_09M',\n",
    "               'fwd_10M', 'fwd_11M', 'fwd_12M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['recognized_', \n",
    "                'service_',\n",
    "                'deferred_B_',\n",
    "                'deferred_1M_',\n",
    "                'deferred_3M_',\n",
    "                'deferred_6M_',\n",
    "                'deferred_1Y_',\n",
    "                'deferred_2Y_',\n",
    "                'deferred_3Y_',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fcst(df_fcst, df_FX_rates, list_columns, new_columns):\n",
    "    \n",
    "    for i in list_columns:\n",
    "        new_column = i+'US'\n",
    "        df_fcst[new_column]= 0\n",
    "    \n",
    "    # get the unique list of currency and BU combinations in the forecast\n",
    "    v_un_BU, v_un_curr = find_unique_curr_and_BU(df_fcst)\n",
    "    for i in range(len(v_un_BU)):\n",
    "        this_BU = v_un_BU[i]\n",
    "        this_curr = v_un_curr[i]\n",
    "        print('working on BU: {0}  and currency: {1}'.format(this_BU, this_curr))\n",
    "        \n",
    "        # create the list of forwards to use here\n",
    "        these_forwards = df_FX_rates[df_FX_rates['DC']==this_curr]\n",
    "        just_forwards = these_forwards[new_columns]\n",
    "        if these_forwards.is_direct.values == 1:\n",
    "            \n",
    "            transp_fwds= just_forwards.transpose(copy=True).values\n",
    "        \n",
    "        else:\n",
    "            transp_fwds = just_forwards.transpose(copy=True).values\n",
    "            transp_fwds = 1/transp_fwds\n",
    "        \n",
    "        this_slice = df_fcst[(df_fcst['BU']==this_BU)&\n",
    "                             (df_fcst['curr']==this_curr)].copy()\n",
    "        \n",
    "        for col in list_columns:\n",
    "            new_column = col+'US'\n",
    "            old_column = col+'DC'\n",
    "            \n",
    "            DC_values =this_slice[old_column].values\n",
    "            DC_values = DC_values.reshape(-1,1)\n",
    "            transp_fwds = transp_fwds.reshape(-1,1)\n",
    "            xx = DC_values * transp_fwds\n",
    "            \n",
    "            df_fcst.loc[(df_fcst['BU']==this_BU)&\n",
    "                           (df_fcst['curr']==this_curr),\n",
    "                           new_column] = xx\n",
    "\n",
    "    return df_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst = convert_fcst(df_fcst, df_FX_rates, list_columns, new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking with a slice of the dataframe df_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_slice = df_fcst[(df_fcst['BU']=='Creative')&\n",
    "                  (df_fcst['curr']=='JPY')]\n",
    "us_slice.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = us_slice['deferred_1Y_DC']\n",
    "us = us_slice['deferred_1Y_US']\n",
    "print(dc/us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging the df_fcst with the df_bililngs for easier charting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billings['is_forecast']= 0\n",
    "df_fcst['is_forecast']=1\n",
    "df = pd.concat([df_billings, df_fcst],\n",
    "            join='outer',\n",
    "            ignore_index=True)\n",
    "df = df.fillna(0)\n",
    "df.sort_values(by=['curr', 'BU', 'period'], inplace=True)\n",
    "\n",
    "input_df_dict_short = {'model_dict': model_dict,\n",
    "                       'ADBE_cal':df_cal,\n",
    "                       'bookings': df_bookings,\n",
    "                       'FX_forwards': df_FX_fwds,\n",
    "                       'FX_rates': df_FX_rates,\n",
    "                       'final': df\n",
    "                }\n",
    "pickle.dump(input_df_dict, open('../data/processed/final_forecast.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
